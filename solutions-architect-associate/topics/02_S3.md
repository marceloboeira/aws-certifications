# S3 101
> Simple Storage Service

### What is S3?

* Object-based Storage
* Redundant/distributed - The data can be spread across multiple devices and regions/AZs.

### Basics

* Files can be from 0 Bytes to 5 TB long
* Unlimited size (since it is distributed it can be flexible)
* Files are stored in "Buckets" (namespace)
* The name of the bucket is universal, meaning it can't be duplicate even across accounts
  * The bucket name generates an URL
      * e.g.: `https://myprettyname.s3.amazonaws.com`
* When you upload a file to S3 you receive a 200 (created) HTTP code

#### Objects

Objects are consisted of:

* Key - Name of the object
* Value - Sequence of bytes that composes the file/stream
* VersionID - Important for versioning, if you want to keep history
* Metadata - Data about your file
* Sub-resources:
 * ACL (Policies/Expiration/Hooks)
 * Torrent

PS: S3 doesn't have paths, or a concept of folder, but if you use `/` in the name of the file, or the key to the object, it can be interpreted by CLI and WebTool as a Path/Folder.

### Block Based vs Object Based Storage

* Block
  * files are split into evenly sized blocks of data, each with its own address but with no additional information (metadata) to provide more context for what that block of data is.
  * Encountered the majority of enterprise workloads, e.g.: "disks"
  * Usage: loading applications, operating systems, filesystems ...
* Object
  * The data can be anything you want to store, from a family photo to a 400,000-page manual for assembling an aircraft.
  * Objects also aren't always directly mappable to files. They may be subfiles (a portion of a file), or simply a collection of bits and bytes related to other and not part of any file.
  * An expandable amount of metadata. The metadata is defined by whoever creates the object storage; it contains contextual information about what the data is, what it should be used for, its confidentiality, or anything else that is relevant to the way in which the data is used.
  * A globally unique identifier. The identifier is a 128-bit unique valuegiven to the object in order for the object to be found over a distributed system. This way, it's possible to find the data without having to know the physical location of the data (which could exist within different parts of a data center or different parts of the world).

> The scalable resiliency of object also creates the challenge of deciding betweeneventual consistency and strong consistency. Eventual consistency is where the latest version of an object will be first stored on one node, and then eventually replicated to its other locations. Strong consistency would require the new version to be immediately replicated.  The strongest consistency would be to delay the write acknowledgment until all copies had been successfully replicated.

> Eventual consistency can provide unlimited scalability. It ensures high availability for data that needs to be durably stored but is relatively static and will not change much, if at all. This is why storing photos, video, and other unstructured data is an ideal use case for object storage systems; it does not need to be constantly altered. The downside to eventual consistency is that there is no guarantee that a read request returns the most recent version of the data.
> Strong consistency is needed for real-time systems such as transactional databases that are constantly being written to, but provide limited scalability and reduced availability as a result of hardware failures. Scalability becomes even more difficult within a geographically distributed system. Strong consistency is a requirement, however, whenever a read request must return the most updated version of the data.

> Technically, both object and block storage can do either evenual or strong consistency, but typically object storage uses strong consistency and object storage tends to use eventual consistency. Therefore, applications where eventual consistency brings value are typically best served by object storage, and those wanting strong consistency tend to prefer block storage.

More:

* [Block vs Object Storage - Differences](https://www.druva.com/blog/object-storage-versus-block-storage-understanding-technology-differences/)


### How does consistency work for S3?

* Read after Write consistency for PUTS of new Objects - Once a confirmation of write (HTTP 200) is given, the object can be read already.
* Eventual Consistency for overwrite PUTS and DELETES (Updates) - If you are to UPDATE or DELETE that document there is no strict guarantee the next read is updated or "empty" for deletes. Eventually it gets consistent.

### Guarantees

* Built for 99.99% availability - around 52m 35s down within 1 year
* Amazon guarantees 99.9% availability - around 8h 45m 56s within 1 year
* Amazon guarantees 99.999999999% (eleven-nines) durability - very unlikely that the file is going to be lost

### Other Features

* Tiered Storage
* LifeCycle Management
* Versioning
* Encryption
* MFA Delete
* ACLs

### Storage Classes

* 1. Standard
  * 99.99% availability
  * 99.999999999% durability
  * it is designed to sustain the loss of 2 facilities at the same time (in case of a natural disaster for instance)
* 2. IA - Infrequently accessed
  * Data that is accessed less regularly but it requires instant access
  * Cheaper than standard but a "retrieval fee" is charged
* 3. OA - One Zone
  * Data that is accessed less regularly but it requires instant access
  * Low cost
  * No replication accross AZ
* 4. Intelligent Tiering
  * It automatically moves from zones based on frequency of access
* 5. Glacier
  * Retrieval times configurable from minutes to hours
  * Really low-cost and durability oriented
* 6. Glacier - Deep Archive
  * Same as Glacier but cheaper due to higher retrieval times
  * Up to 12 Hours (configurable)

More info:

* [AWS S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/)

### Charges

* Storage - How much data you are storing
* Requests - How frequently do you write/read
* Data Transfer - Size of upload/download from the requests
* Storage Management - Types of storage and its requirements/features
* Transfer Acceleration - Files are uploaded to edge locations for faster access
* Cross Region Replication - Geographical location replication transfer/data rates


TODO:
- READ THE FAQ

### Basics Lab

1. Create a bucket
2. Create a file within the bucket
3. Make the file / bucket publicly accessible

## Security & Encryption

* By default ACL is set to private
* You can configute access logs to another S3 bucket with all requests

### Encryption

* In Transit - SSL/TLS
* At Rest - Content is encrypted
  * Server Side
    * S3 Managed Keys - SSE-S3
    * AWS Key Management Service - SSE-KMS
    * Customer Provided Keys - SSE-C
  * Client Side
    * You encrypt data on your end and upload encrypted objects, decrypting on read

LAB
* Add KMS to Objects for testing

## Versioning

* Store all versions (including all writes of deleted objects)
* Increases price (by a lot if you are managing large files)
* Once Enabled it cannot be disabled, only suspended
  * You need to delete and re-create the bucket to disable it
* It integrate with lifecycle rules e.g.: delete all versions older than 15 days
* MFA delete capability

## Lifecycle Management LAB

* Crete lifcycle-rules
* Test versions

## Cross-Account Access

3 different ways:
* Using Bucket Policy (applies to the entire bucket)
* Using Bucket ACLs & IAM (individual objects) API Only
* Cross-account IAM Roles, API and Console.

## Practice with Cross Account Roles

* Create Policy
* Create Assume Role
* Attach Policy
* Create a IAM user account on the 3party orgnaization (might have to set a password)
* LogIn to the new user
* Assume Role: https://signin.aws.amazon.com/switchrole?roleName=s3-cross-account&account=<account_id>
* You can try to do other things, for instance, EC2, to make sure you don't have access

## Cross-Region Replication

* It requires versioning to be enabled (on the source and destination buckets)
* It doesn't replicate existent content, only future content (automatically). You are able to request AWS support team on replicating big buckets by opening a ticket on the Support Tool for "Replication for Existing Objects".
* It doesn't replicate deletes across buckets
* It doesn't replicate some encrypted objects
* It doesn't replicate some types of storage (glacier/deep-archive)

Links:

* [What isn't replicated?](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-what-is-isnot-replicated.html#replication-what-is-not-replicated)
* [Replication Configuration Overview](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-add-config.html)
* [Cross-Region Replication Monitor](https://aws.amazon.com/solutions/cross-region-replication-monitor/)

## Transfer Acceleration

You use a edge location to upload your files and it gets replicated across AWS Backbone Network.

AWS has a test page to calculate the speed on how this tool can be faster than using normal S3 endpoint links: https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html.

## Quiz

* IA Avail is 99.50%
* S3 Styles of URL - `https://s3-<region>.amazonaws.com/<bucket_name>`
* S3 Upload limits (PUT and Size Limits?)
* Glacier Retrieval Types
* 100 buckets by default
* 3500 PUTs/s is the limit since 2008
* MultiPart Upload

### Path Types

* Virtual Hosted Style Access: `https://<bucket-name>.s3.<region>.amazonaws.com/<key>`
* Path-Style Access: `https://s3.<region>.amazonaws.com/<bucket-name>/<key>`
* Accessing a Bucket using S3://: `S3://<bucket-name>/<key>`

Buckets created after September 30, 2020, will support only virtual hosted-style requests. Path-style requests will continue to be supported for buckets created on or before this date
> Source: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html


### Limitations

Q: How much data can I store in Amazon S3?

The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes. The largest object that can be uploaded in a single PUT is 5 gigabytes. For objects larger than 100 megabytes, customers should consider using the Multipart Upload capability.

Q: How many buckets can I have?

100 (by default, more if asked)

### From FAQs

#### S3 Classes SLA

Q: How reliable is Amazon S3?

Amazon S3 gives any developer access to the same highly scalable, highly available, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. The S3 Standard storage class is designed for 99.99% availability, the S3 Standard-IA storage class is designed for 99.9% availability, the S3 One Zone-IA storage class is designed for 99.5% availability, and the S3 Glacier and S3 Glacier Deep Archive class are designed for 99.99% availability and SLA of 99.9%. All of these storage classes are backed by the Amazon S3 Service Level Agreement.
> Source https://aws.amazon.com/s3/sla/

Q:  How durable is Amazon S3?

Amazon S3 Standard, S3 Standard-IA, S3 One Zone-IA, and S3 Glacier are all designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects.
For example, if you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years. In addition, Amazon S3 Standard, S3 Standard-IA, and S3 Glacier are all designed to sustain data in the event of an entire S3 Availability Zone loss.

As with any environment, the best practice is to have a backup and to put in place safeguards against malicious or accidental deletion. For S3 data, that best practice includes secure access permissions, Cross-Region Replication, versioning, and a functioning, regularly tested backup.

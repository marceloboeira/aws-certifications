# S3 101
> Simple Storage Service

### What is S3?

* Object-based Storage
* Redundant/distributed - The data can be spread across multiple devices and regions/AZs.

### Basics

* Files can be from 0 Bytes to 5 TB long
* Unlimited size (since it is distributed it can be flexible)
* Files are stored in "Buckets" (namespace)
* The name of the bucket is universal, meaning it can't be duplicate even across accounts
  * The bucket name generates an URL
      * e.g.: `https://myprettyname.s3.amazonaws.com`
* When you upload a file to S3 you receive a 200 (created) HTTP code

#### Objects

Objects are consisted of:

* Key - Name of the object
* Value - Sequence of bytes that composes the file/stream
* VersionID - Important for versioning, if you want to keep history
* Metadata - Data about your file
* Sub-resources:
 * ACL (Policies/Expiration/Hooks)
 * Torrent

PS: S3 doesn't have paths, or a concept of folder, but if you use `/` in the name of the file, or the key to the object, it can be interpreted by CLI and WebTool as a Path/Folder.

### Block Based vs Object Based Storage

* Block
  * files are split into evenly sized blocks of data, each with its own address but with no additional information (metadata) to provide more context for what that block of data is.
  * Encountered the majority of enterprise workloads, e.g.: "disks"
  * Usage: loading applications, operating systems, filesystems ...
* Object
  * The data can be anything you want to store, from a family photo to a 400,000-page manual for assembling an aircraft.
  * Objects also aren't always directly mappable to files. They may be subfiles (a portion of a file), or simply a collection of bits and bytes related to other and not part of any file.
  * An expandable amount of metadata. The metadata is defined by whoever creates the object storage; it contains contextual information about what the data is, what it should be used for, its confidentiality, or anything else that is relevant to the way in which the data is used.
  * A globally unique identifier. The identifier is a 128-bit unique valuegiven to the object in order for the object to be found over a distributed system. This way, it's possible to find the data without having to know the physical location of the data (which could exist within different parts of a data center or different parts of the world).

> The scalable resiliency of object also creates the challenge of deciding betweeneventual consistency and strong consistency. Eventual consistency is where the latest version of an object will be first stored on one node, and then eventually replicated to its other locations. Strong consistency would require the new version to be immediately replicated.  The strongest consistency would be to delay the write acknowledgment until all copies had been successfully replicated.

> Eventual consistency can provide unlimited scalability. It ensures high availability for data that needs to be durably stored but is relatively static and will not change much, if at all. This is why storing photos, video, and other unstructured data is an ideal use case for object storage systems; it does not need to be constantly altered. The downside to eventual consistency is that there is no guarantee that a read request returns the most recent version of the data.
> Strong consistency is needed for real-time systems such as transactional databases that are constantly being written to, but provide limited scalability and reduced availability as a result of hardware failures. Scalability becomes even more difficult within a geographically distributed system. Strong consistency is a requirement, however, whenever a read request must return the most updated version of the data.

> Technically, both object and block storage can do either evenual or strong consistency, but typically object storage uses strong consistency and object storage tends to use eventual consistency. Therefore, applications where eventual consistency brings value are typically best served by object storage, and those wanting strong consistency tend to prefer block storage.

More:

* [Block vs Object Storage - Differences](https://www.druva.com/blog/object-storage-versus-block-storage-understanding-technology-differences/)


### How does consistency work for S3?

* Read after Write consistency for PUTS of new Objects - Once a confirmation of write (HTTP 200) is given, the object can be read already.
* Eventual Consistency for overwrite PUTS and DELETES (Updates) - If you are to UPDATE or DELETE that document there is no strict guarantee the next read is updated or "empty" for deletes. Eventually it gets consistent.

### Guarantees

* Built for 99.99% availability - around 52m 35s down within 1 year
* Amazon guarantees 99.9% availability - around 8h 45m 56s within 1 year
* Amazon guarantees 99.999999999% (eleven-nines) durability - very unlikely that the file is going to be lost

### Other Features

* Tiered Storage
* LifeCycle Management
* Versioning
* Encryption
* MFA Delete
* ACLs

### Storage Classes

* 1. Standard
  * 99.99% availability
  * 99.999999999% durability
  * it is designed to sustain the loss of 2 facilities at the same time (in case of a natural disaster for instance)
* 2. IA - Infrequently accessed
  * Data that is accessed less regularly but it requires instant access
  * Cheaper than standard but a "retrieval fee" is charged
* 3. OA - One Zone
  * Data that is accessed less regularly but it requires instant access
  * Low cost
  * No replication accross AZ
* 4. Intelligent Tiering
  * It automatically moves from zones based on frequency of access
* 5. Glacier
  * Retrieval times configurable from minutes to hours
  * Really low-cost and durability oriented
* 6. Glacier - Deep Archive
  * Same as Glacier but cheaper due to higher retrieval times
  * Up to 12 Hours (configurable)

More info:

* [AWS S3 Storage Classes](https://aws.amazon.com/s3/storage-classes/)

### Charges

* Storage - How much data you are storing
* Requests - How frequently do you write/read
* Data Transfer - Size of upload/download from the requests
* Storage Management - Types of storage and its requirements/features
* Transfer Acceleration - Files are uploaded to edge locations for faster access
* Cross Region Replication - Geographical location replication transfer/data rates


TODO:
- READ THE FAQ

### Basics Lab

1. Create a bucket
2. Create a file within the bucket
3. Make the file / bucket publicly accessible

## Security & Encryption

* By default ACL is set to private
* You can configute access logs to another S3 bucket with all requests

### Encryption

* In Transit - SSL/TLS
* At Rest - Content is encrypted
  * Server Side
    * S3 Managed Keys - SSE-S3
    * AWS Key Management Service - SSE-KMS
    * Customer Provided Keys - SSE-C
  * Client Side
    * You encrypt data on your end and upload encrypted objects, decrypting on read

LAB
* Add KMS to Objects for testing

## Versioning

* Store all versions (including all writes of deleted objects)
* Increases price (by a lot if you are managing large files)
* Once Enabled it cannot be disabled, only suspended
  * You need to delete and re-create the bucket to disable it
* It integrate with lifecycle rules e.g.: delete all versions older than 15 days
* MFA delete capability

## Lifecycle Management LAB

* Crete lifcycle-rules
* Test versions

## Cross-Account Access

3 different ways:
* Using Bucket Policy (applies to the entire bucket)
* Using Bucket ACLs & IAM (individual objects) API Only
* Cross-account IAM Roles, API and Console.
